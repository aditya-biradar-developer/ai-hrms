# AI Service Configuration
PORT=5001
FLASK_ENV=development

# ðŸš€ GROQ AI (ULTRA-FAST & FREE!)
# Get your free API key: https://console.groq.com/
# Best for question generation - 200-500 tokens/sec!
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.1-70b-versatile

# ðŸŒŸ Google Gemini AI (FREE, FAST, HIGH QUALITY!)
# Get your free API key: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_api_key_here

# ðŸš€ HuggingFace Inference API (FASTEST for PDF extraction!)
# Get free API key: https://huggingface.co/settings/tokens
# Optional - works without key but with rate limits
HUGGINGFACE_API_KEY=your_hf_token_here

# LLM Configuration (Fallback if Gemini not configured)
# Set to 'true' to use Ollama (recommended), 'false' to use llama-cpp-python
USE_OLLAMA=true

# Ollama Configuration (if USE_OLLAMA=true)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama2:7b-chat

# llama-cpp-python Configuration (if USE_OLLAMA=false)
MODEL_PATH=./models/llama-2-7b-chat.gguf

# Generation Parameters
MAX_TOKENS=2048
TEMPERATURE=0.7
TOP_P=0.95

# API Configuration
BACKEND_URL=http://localhost:5000
API_SECRET=8b00c67b4089974971de5983a26e8daa866dfeecc09c1733636681d9fcade593

# Vector Store
CHROMA_DB_PATH=./data/chroma_db
